{"cells":[{"cell_type":"markdown","metadata":{"id":"JGgvojIKJYHF"},"source":["<h1>XGBoost</h1>"]},{"cell_type":"markdown","metadata":{"id":"8tOLBiOiIvzW"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"1_95sQo3JIlI"},"source":["XGBoost is a gradient boosting model which operates by building a series of decision trees iteratively, where each subsequent tree will try to correct the errors made by the previous tree. It uses gradient-based optimization technique to minimize the loss function."]},{"cell_type":"markdown","metadata":{"id":"eynTwbU9kj3s"},"source":["# How XGBoost algorithm works"]},{"cell_type":"markdown","metadata":{"id":"omJGd9PlkqrW"},"source":["## 1. Calculate pseudo residuals\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jcBBVUAMn3c7"},"source":["$Residuals = y - p$  \n","* $y$: Target labels\n","* $p$: Prediction probability  \n","\n","The initial prediction probability is usually 0.5\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l3G3aBpBqbga"},"source":["## 2. Find the best split"]},{"cell_type":"markdown","metadata":{"id":"arFiyWV5sW6A"},"source":["XGBoost uses a metric called **Similarity Score** to choose the split when building the decision tree.  \n","\n","*Similarity Score* = $\\frac{∑(Residuals)^2}{∑Pi-1*(1-Pi-1) + λ}$  \n","\n","* $Pi$: Previous prediction probability\n","* $λ$: Regularization parameter\n","\n","The feature and value decided as the threshold to split the tree is the one having the highest **Gain**  \n","\n","$Gain = SimilarityScore(LeftChild) + SimilarityScore(RightChild) - SimilarityScore(Root)$"]},{"cell_type":"markdown","metadata":{"id":"VfRfrQbsBVNS"},"source":["## 3. Compute the output value"]},{"cell_type":"markdown","metadata":{"id":"niC5SbFyBaDX"},"source":["The value stored in leaf node are computed using the formular below\n","\n","$Output = \\frac{∑(Residuals)}{∑Pi-1*(1-Pi-1) + λ}$"]},{"cell_type":"markdown","metadata":{"id":"qGDn_lL0Q_6q"},"source":["## 4. Update prediction probability"]},{"cell_type":"markdown","metadata":{"id":"9Nowi8rbRdG4"},"source":["After finishing building the tree, the prediction will be updated based on the calculated output values  \n","\n","$Prediction = ln(\\frac{P}{1-P}) + α*Output$\n","\n","* $P$: Previous prediction probabiblity\n","* $Output$: The computed output value of the current tree  \n","\n","The values we get from the formular above actually are log odds values. Therefore, they need to be converted to probability values\n","\n","$Probability = \\frac{e^{pred}}{1 + e^{pred}}$  \n","\n","* $pred$: The log odds values of prediction  \n","\n","Compute the residuals again with the updated probability and repeat the same steps above for subsequent models\n"]},{"cell_type":"markdown","metadata":{"id":"hgJQ0ddhnxuo"},"source":["# Implementation"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:42:05.389537Z","iopub.status.busy":"2024-05-06T01:42:05.389167Z","iopub.status.idle":"2024-05-06T01:42:08.849731Z","shell.execute_reply":"2024-05-06T01:42:08.848697Z","shell.execute_reply.started":"2024-05-06T01:42:05.389508Z"},"id":"jjLREnOAUH6r","trusted":true},"outputs":[],"source":["import numpy as np\n","import time\n","import math\n","from sklearn.metrics import accuracy_score, log_loss\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"x9-4tSGH70vL"},"source":["## Implement decision tree model"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:42:22.572577Z","iopub.status.busy":"2024-05-06T01:42:22.572061Z","iopub.status.idle":"2024-05-06T01:42:22.594714Z","shell.execute_reply":"2024-05-06T01:42:22.592240Z","shell.execute_reply.started":"2024-05-06T01:42:22.572546Z"},"id":"NWGpSJ29bVr4","trusted":true},"outputs":[],"source":["class Tree:\n","    def __init__(self, max_depth = 3, min_samples = 1, min_child_weight = 1, lambda_ = 0, gamma = 0):\n","        self.max_depth = max_depth\n","        self.min_samples = min_samples\n","        self.min_child_weight = min_child_weight\n","        self.lambda_ = lambda_\n","        self.gamma = gamma\n","        self.tree = {}\n","        self.fbs_time = 0\n","\n","\n","    #Compute the Similarity Score\n","    def similarity(self, residual, probs):\n","        nu = np.sum(residual) ** 2\n","        de = np.sum(probs * (1 - probs)) + self.lambda_\n","        return nu / de\n","\n","    #Compute the output value in leaf node\n","    def compute_output(self, residual, probs):\n","        nu = np.sum(residual)\n","        de = np.sum(probs * (1 - probs)) + self.lambda_\n","        return nu / de\n","\n","    def cover(self, probs):\n","        return np.sum(probs * (1 - probs))\n","\n","    def split_data(self, X, feature_idx, split_value):\n","        left_idx = X[:, feature_idx] <= split_value\n","        right_idx = X[:, feature_idx] > split_value\n","        return left_idx, right_idx\n","\n","    #Choose the best split based on the Gain score\n","    def find_best_split(self, X, residual, probs):\n","        best_gain = -np.inf\n","        best_split_feature_idx = None\n","        best_split_value = None\n","\n","        for feature_idx in range(X.shape[1]):\n","            list_values = X[:, feature_idx]\n","            list_unique = np.unique(list_values)\n","\n","            for i in range(len(list_unique) - 1):\n","                value = (list_unique[i] + list_unique[i + 1]) / 2\n","\n","                left_idx, right_idx = self.split_data(X, feature_idx, value)\n","                p_left = probs[left_idx]\n","                p_right = probs[right_idx]\n","\n","                if (len(left_idx) < self.min_samples or len(right_idx) < self.min_samples\n","                    or self.cover(p_left) < self.min_child_weight or self.cover(p_right) < self.min_child_weight):\n","                    continue\n","\n","                r_left = residual[left_idx]\n","                r_right = residual[right_idx]\n","\n","                gain = self.similarity(r_left, p_left) + self.similarity(r_right, p_right) - self.similarity(residual, probs)\n","\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    best_split_feature_idx = feature_idx\n","                    best_split_value = value\n","\n","        if(best_gain - self.gamma < 0):\n","            best_split_feature_idx = None\n","            best_split_value = None\n","\n","        return best_split_feature_idx, best_split_value\n","\n","    def build_tree(self, X, residual, probs, depth):\n","        if depth >= self.max_depth or len(X) <= self.min_samples:\n","            return self.compute_output(residual, probs)\n","\n","        start = time.time()\n","        split_feature_idx, split_value = self.find_best_split(X, residual, probs)\n","        end = time.time()\n","        self.fbs_time += (end - start)\n","\n","        if split_feature_idx is None:\n","            return self.compute_output(residual, probs)\n","\n","        left_idx, right_idx = self.split_data(X, split_feature_idx, split_value)\n","        left = self.build_tree(X[left_idx], residual[left_idx], probs[left_idx], depth + 1)\n","        right = self.build_tree(X[right_idx], residual[right_idx], probs[right_idx], depth + 1)\n","\n","        self.tree = {\n","            'split_feature_idx': split_feature_idx,\n","            'split_value': split_value,\n","            'left_child': left,\n","            'right_child': right\n","        }\n","        return self.tree\n","\n","    def get_output(self, x, tree):\n","        if isinstance(tree, dict):\n","            split_feature_idx = tree['split_feature_idx']\n","            split_value = tree['split_value']\n","            if x[split_feature_idx] <= split_value:\n","                return self.get_output(x, tree['left_child'])\n","            else:\n","                return self.get_output(x, tree['right_child'])\n","        else:\n","            return tree\n","\n","    def fit(self, X, residual, probs):\n","        depth = 0\n","        self.tree = self.build_tree(X, residual, probs, depth)\n","\n","    def predict(self, X):\n","        return np.array([self.get_output(x, self.tree) for x in X])"]},{"cell_type":"markdown","metadata":{"id":"erREaDng8IYj"},"source":["## Sequential XGBoost model"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:42:27.567381Z","iopub.status.busy":"2024-05-06T01:42:27.566855Z","iopub.status.idle":"2024-05-06T01:42:27.587080Z","shell.execute_reply":"2024-05-06T01:42:27.585622Z","shell.execute_reply.started":"2024-05-06T01:42:27.567340Z"},"trusted":true},"outputs":[],"source":["class XGBoost:\n","    def __init__(self, n_estimators, lr, lambda_ = 1e-7, gamma = 0, min_child_weight = 1, max_depth = 3):\n","        self.n_estimators = n_estimators\n","        self.lr = lr\n","        self.initial_pred = 0.5\n","        self.lambda_ = lambda_\n","        self.min_child_weight = min_child_weight\n","        self.max_depth = max_depth\n","        self.gamma = gamma\n","        self.models = []\n","        self.fbs_time = 0\n","        self.logodds_time = 0\n","        self.residual_time = 0\n","        self.predict_time = 0\n","\n","    #Turn probability into log odds value\n","    def compute_logodds(self, p):\n","        return np.log(p / (1 - p))\n","\n","    #Caclculate pseudo residuals\n","    def residual(self, y_true, y_pred):\n","        return (y_true - y_pred)\n","    \n","    #Change log odds value back to probability\n","    def compute_prob(self, logodds_p):\n","        return np.exp(logodds_p) / (1 + np.exp(logodds_p))\n","\n","    def fit(self, X, y):\n","        p = np.full(len(y), self.initial_pred)\n","\n","        for _ in range(self.n_estimators):\n","            probs = np.copy(p)\n","            start = time.time()\n","            residual = self.residual(y, p)\n","            end = time.time()\n","            self.residual_time += (end - start)\n","\n","            model = Tree(lambda_ = self.lambda_, gamma = self.gamma, max_depth = self.max_depth, min_child_weight = self.min_child_weight)\n","            model.fit(X, residual, probs)\n","            self.fbs_time += model.fbs_time\n","\n","            start = time.time()\n","            log_odds = self.compute_logodds(p)\n","            end = time.time()\n","            self.logodds_time += (end - start)\n","\n","            start = time.time()\n","            logodds_p = log_odds + self.lr * model.predict(X)\n","            p = self.compute_prob(logodds_p)\n","            end = time.time()\n","            self.predict_time += (end - start)\n","\n","            self.models.append(model)\n","\n","    def predict_proba(self, X):\n","        pred = np.full(len(X), self.initial_pred)\n","        for model in self.models:\n","            logodds_p = self.compute_logodds(pred) + self.lr * model.predict(X)\n","            pred = np.exp(logodds_p) / (1 + np.exp(logodds_p))\n","        return pred"]},{"cell_type":"markdown","metadata":{"id":"b1fZjrmlxvUH"},"source":["## Load minimal data with 3000 rows and 300 rows for training and testing data respectively"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1x2IQyQTxvUI"},"outputs":[],"source":["minimal_train = np.load('train_data_3labels.npz', allow_pickle = True)\n","X_train = minimal_train['data']\n","y_train = minimal_train['label']\n","\n","minimal_test = np.load('test_data_3labels.npz', allow_pickle = True)\n","X_test = minimal_test['data']\n","y_test = minimal_test['label']"]},{"cell_type":"markdown","metadata":{"id":"UWpVfHCBxvUI"},"source":["## Train and evaluate the model"]},{"cell_type":"markdown","metadata":{"id":"PHFyK2n--toa"},"source":["Since our XGBoost implementation is only able to perform binary classification at the moment, we will use the One vs Rest (OvR) strategy to deal with multiple classification problem. The OvR strategy splits the multiclass dataset into multiple binary classification problems per label. The model is then trained on each binary classfication problem and the predictions are made by choosing the label having the highest prediction probability."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:43:15.921821Z","iopub.status.busy":"2024-05-06T01:43:15.921412Z","iopub.status.idle":"2024-05-06T01:43:15.927005Z","shell.execute_reply":"2024-05-06T01:43:15.925939Z","shell.execute_reply.started":"2024-05-06T01:43:15.921793Z"},"trusted":true},"outputs":[],"source":["def extract_time_by_min_sec(time):\n","    minute = math.floor(time / 60)\n","    second = round(time % 60)\n","    return (minute, second)"]},{"cell_type":"markdown","metadata":{"id":"Da8xN0oYV5D1"},"source":["### Binary classification"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T02:13:01.899780Z","iopub.status.busy":"2024-05-06T02:13:01.898845Z","iopub.status.idle":"2024-05-06T02:18:58.964042Z","shell.execute_reply":"2024-05-06T02:18:58.962742Z","shell.execute_reply.started":"2024-05-06T02:13:01.899734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9466666666666667\n","Log logss:  1.922328180752915\n","Total time in minutes: 5m 57s\n","Total time in seconds: 357.0480463504791 seconds\n"]}],"source":["binary_classifier = XGBoost(n_estimators = 3, lr = 0.3)\n","start_time = time.time()\n","binary_classifier.fit(X_train, (y_train == 0).astype(int))\n","end_time = time.time()\n","\n","y_prob_pred = binary_classifier.predict_proba(X_test)\n","binary_labels_pred = (y_prob_pred > 0.5).astype(int)\n","binary_labels_test = (y_test == 0).astype(int)\n","\n","accuracy_binary = accuracy_score(binary_labels_test, binary_labels_pred)\n","log_loss_val = log_loss(binary_labels_test, binary_labels_pred)\n","time_binary = end_time - start_time\n","time_binary_in_minute = extract_time_by_min_sec(time_binary)\n","\n","print('Accuracy:', accuracy_binary)\n","print('Log logss: ', log_loss_val)\n","print(f'Total time in minutes: {time_binary_in_minute[0]}m {time_binary_in_minute[1]}s')\n","print(f'Total time in seconds: {time_binary} seconds')"]},{"cell_type":"markdown","metadata":{"id":"swU683eTElkS"},"source":["### Multi-classification"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:43:22.655405Z","iopub.status.busy":"2024-05-06T01:43:22.654656Z","iopub.status.idle":"2024-05-06T01:43:22.663700Z","shell.execute_reply":"2024-05-06T01:43:22.662263Z","shell.execute_reply.started":"2024-05-06T01:43:22.655358Z"},"trusted":true},"outputs":[],"source":["class MultiClassifier:\n","    def __init__(self, n_estimators = 3, lr = 0.3):\n","        self.models = []\n","        self.n_estimators = n_estimators\n","        self.lr = lr\n","        self.training_time = 0\n","\n","    def fit(self, X, y):\n","        start_time = time.time()\n","        for label in np.unique(y):\n","            binary_labels = (y == label).astype(int)\n","            model = XGBoost(self.n_estimators, self.lr)\n","            model.fit(X, binary_labels)\n","            self.models.append(model)\n","        end_time = time.time()\n","        self.training_time += (end_time - start_time)\n","\n","    def predict(self, X):\n","        preds = []\n","        for model in self.models:\n","            preds.append(model.predict_proba(X))\n","        return np.argmax(preds, axis = 0)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T01:43:48.124939Z","iopub.status.busy":"2024-05-06T01:43:48.124513Z","iopub.status.idle":"2024-05-06T02:01:30.297404Z","shell.execute_reply":"2024-05-06T02:01:30.296101Z","shell.execute_reply.started":"2024-05-06T01:43:48.124909Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9566666666666667\n","Total time in minutes: 17m 42s\n","Total time: 1062.1561212539673 seconds\n"]}],"source":["multi_classifier = MultiClassifier()\n","multi_classifier.fit(X_train, y_train)\n","\n","y_pred = multi_classifier.predict(X_test)\n","accuracy_3labels = accuracy_score(y_test, y_pred)\n","time_3labels = multi_classifier.training_time\n","time_3labels_in_minute = extract_time_by_min_sec(time_3labels)\n","\n","print('Accuracy:', accuracy_3labels)\n","print(f'Total time in minutes: {time_3labels_in_minute[0]}m {time_3labels_in_minute[1]}s')\n","print(f'Total time: {time_3labels} seconds')"]},{"cell_type":"markdown","metadata":{},"source":["## Save result"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["result_df = pd.read_csv('result.csv')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["result_df['Sequential_training_time'] = [time_binary, time_3labels, None]\n","result_df['Sequential_accuracy'] = [accuracy_binary, accuracy_3labels, None]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["result_df.to_csv('result.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["## Analysis"]},{"cell_type":"markdown","metadata":{},"source":["Analyze the execution time of each main phase to see which part takes the most time"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T02:12:28.252976Z","iopub.status.busy":"2024-05-06T02:12:28.252180Z","iopub.status.idle":"2024-05-06T02:12:28.262718Z","shell.execute_reply":"2024-05-06T02:12:28.261337Z","shell.execute_reply.started":"2024-05-06T02:12:28.252932Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Find_best_split total time: 1061.9381017684937\n","Compute residual total time: 0.0001990795135498047\n","Compute logodds total time: 0.0004916191101074219\n","Compute probality total time: 0.057578325271606445\n"]}],"source":["fbs_time = sum([multi_classifier.models[i].fbs_time for i in range(3)])\n","residual_time = sum([multi_classifier.models[i].residual_time for i in range(3)])\n","logodds_time = sum([multi_classifier.models[i].logodds_time for i in range(3)])\n","predict_time = sum([multi_classifier.models[i].predict_time for i in range(3)]) \n","print('Find_best_split total time:', fbs_time)\n","print('Compute residual total time:', residual_time)\n","print('Compute logodds total time:', logodds_time)\n","print('Compute probality total time:', predict_time)"]},{"cell_type":"markdown","metadata":{},"source":["![Data](analysis.jpg)"]},{"cell_type":"markdown","metadata":{},"source":["It can be seen that the `find_best_split()` function in the decision tree building phase takes the most time among the phases. Therefore, the main goal of parallelization is to successfully parallelize the `find_best_split()` function, reducing execution time while still ensuring accuracy."]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4946601,"sourceId":8330622,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":4}
